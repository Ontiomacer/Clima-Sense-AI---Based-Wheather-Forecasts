# ClimaSense AI Backend

FastAPI-based AI backend integrating **AgriBERT** and **Gemma** models for agricultural analysis and recommendations.

## ðŸš€ Quick Start

### 1. Setup
```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Server
```bash
python main.py
```

Server starts on `http://localhost:8000`

### 3. Test Endpoints
```bash
python test_endpoints.py
```

---

## ðŸ“¡ API Endpoints

### 1. POST /api/analyze-farm
Analyze farm/crop/soil/weather conditions using AgriBERT classifier.

**Request:**
```json
{
  "text": "soil is dry and temperature is rising"
}
```

**Response:**
```json
{
  "model": "AgriBERT",
  "prediction": "Drought Stress",
  "confidence": 0.91,
  "timestamp": "2025-11-09T10:30:00"
}
```

**Predictions:**
- Drought Stress
- Pest Infestation
- Nutrient Deficiency
- Optimal Conditions
- Waterlogging
- Disease Risk
- Heat Stress
- Cold Stress

---

### 2. POST /api/ai-recommend
Get AI-driven farming recommendations using Gemma model.

**Request:**
```json
{
  "prompt": "Suggest crops suitable for high humidity and low sunlight."
}
```

**Response:**
```json
{
  "model": "Gemma",
  "recommendation": "Rice, sugarcane, and jute thrive in such conditions. These crops are well-adapted to waterlogged conditions and can tolerate shade...",
  "timestamp": "2025-11-09T10:30:00"
}
```

---

### 3. GET /api/health
Check backend health and model loading status.

**Response:**
```json
{
  "status": "ok",
  "models_loaded": true,
  "device": "cpu",
  "agri_classifier": "loaded",
  "gemma_model": "loaded"
}
```

---

### 4. GET /
API information and documentation.

---

## ðŸ§ª Testing

### Using cURL

**Test analyze-farm:**
```bash
curl -X POST http://localhost:8000/api/analyze-farm \
  -H "Content-Type: application/json" \
  -d "{\"text\":\"soil is dry and temperature is rising\"}"
```

**Test ai-recommend:**
```bash
curl -X POST http://localhost:8000/api/ai-recommend \
  -H "Content-Type: application/json" \
  -d "{\"prompt\":\"Suggest crops for high humidity\"}"
```

**Test health:**
```bash
curl http://localhost:8000/api/health
```

### Using Python
```python
import requests

# Analyze farm conditions
response = requests.post(
    "http://localhost:8000/api/analyze-farm",
    json={"text": "crops showing yellow leaves"}
)
print(response.json())

# Get recommendations
response = requests.post(
    "http://localhost:8000/api/ai-recommend",
    json={"prompt": "Best crops for drought areas?"}
)
print(response.json())
```

### Using Test Script
```bash
python test_endpoints.py
```

---

## ðŸ”§ Configuration

### Environment Variables
Create `.env` file:
```env
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO
HUGGINGFACE_API_KEY=your_key_here  # Optional
```

### Model Caching
Models are automatically cached in `./.model_cache/` after first download.

**First run:** 5-10 minutes (downloads ~3GB)
**Subsequent runs:** 30-60 seconds (loads from cache)

---

## ðŸŒ CORS Configuration

Configured to allow:
- `http://localhost:5170` (local development)
- `http://localhost:3000` (alternative local)
- All origins (for development - restrict in production)

---

## ðŸ“Š Models

### AgriBERT Classifier
- **Model:** `GautamR/agri_bert_classifier`
- **Type:** BERT-based sequence classification
- **Size:** ~420MB
- **Purpose:** Classify agricultural conditions
- **Inference:** ~100-200ms per request

### Gemma Recommendation
- **Model:** `pjh11098/Weather_Forecast-Based_Personalized_Recommendation_System_with_Gemma_Model`
- **Type:** Causal language model
- **Size:** ~2.5GB
- **Purpose:** Generate farming recommendations
- **Inference:** ~1-3s per request (CPU), ~200-500ms (GPU)

---

## ðŸš€ Performance

### Speed
- **Classification:** 100-200ms
- **Recommendation:** 1-3s (CPU), 200-500ms (GPU)
- **First request:** Slower (model loading)
- **Subsequent:** Faster (models cached in memory)

### Resource Usage
- **RAM:** 2-4GB (models loaded)
- **Disk:** 3GB (model cache)
- **CPU:** Moderate during inference

### GPU Acceleration
To use GPU (5-10x faster):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## ðŸ”„ Fallback Mode

If models fail to load, the backend automatically uses rule-based fallbacks:

**AgriBERT Fallback:** Keyword-based classification
**Gemma Fallback:** Template-based recommendations

This ensures the API always returns responses even without models.

---

## ðŸ“ Logging

All requests are logged to console:
```
ðŸ“¥ /api/analyze-farm - Request: soil is dry and temperature...
   âœ… AgriBERT prediction: Drought Stress (0.91)

ðŸ“¥ /api/ai-recommend - Request: Suggest crops for high humidity...
   âœ… Gemma generated 245 chars
```

---

## ðŸ› Troubleshooting

### Models not loading
**Issue:** First run takes 5-10 minutes
**Solution:** Wait for models to download. Check internet connection.

### Out of memory
**Issue:** System runs out of RAM
**Solution:** Close other applications or use smaller batch sizes.

### Slow responses
**Issue:** Inference takes too long
**Solution:** 
- First request is always slower (model loading)
- Use GPU for 5-10x speedup
- Consider Hugging Face Inference API

### Connection refused
**Issue:** Frontend can't connect
**Solution:**
- Check backend is running: `http://localhost:8000/health`
- Verify CORS settings
- Check firewall

---

## ðŸ“¦ Project Structure

```
ai-backend/
â”œâ”€â”€ main.py              # FastAPI application
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ test_endpoints.py    # Test script
â”œâ”€â”€ .env.example        # Environment template
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .model_cache/       # Model cache (auto-created)
```

---

## ðŸ” Security

- API keys stored in `.env` (not committed)
- CORS configured for specific domains
- Input validation with Pydantic
- Error handling for all endpoints

---

## ðŸ“š Integration Example

### Frontend (React/TypeScript)
```typescript
// Analyze farm conditions
const analyzeFarm = async (text: string) => {
  const response = await fetch('http://localhost:8000/api/analyze-farm', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text })
  });
  return await response.json();
};

// Get AI recommendations
const getRecommendation = async (prompt: string) => {
  const response = await fetch('http://localhost:8000/api/ai-recommend', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ prompt })
  });
  return await response.json();
};

// Usage
const result = await analyzeFarm("soil is dry");
console.log(result.prediction); // "Drought Stress"

const advice = await getRecommendation("Best crops for drought?");
console.log(advice.recommendation);
```

---

## ðŸŽ¯ Use Cases

### 1. Farm Condition Analysis
Input: "Heavy rainfall causing standing water"
Output: Waterlogging detection with confidence score

### 2. Crop Recommendations
Input: "Suggest crops for high humidity and low sunlight"
Output: AI-generated crop suggestions with reasoning

### 3. Pest Management
Input: "Insects eating leaves of plants"
Output: Pest infestation detection + organic control recommendations

### 4. Weather-Based Advice
Input: "Best practices for drought-prone areas"
Output: Comprehensive drought management strategies

---

## ðŸ“ž Support

### Check Status
```bash
curl http://localhost:8000/api/health
```

### View Logs
Check terminal where `python main.py` is running

### Report Issues
Include:
- Error message
- Request payload
- System info (OS, Python version)
- Backend logs

---

## ðŸ”® Future Enhancements

- [ ] Batch processing endpoints
- [ ] WebSocket support for streaming
- [ ] Response caching
- [ ] Rate limiting
- [ ] API authentication
- [ ] Model fine-tuning endpoints
- [ ] Multi-language support

---

## ðŸ“„ License

Models are subject to their respective licenses:
- AgriBERT: Apache 2.0
- Gemma: Gemma Terms of Use

Backend code: MIT License

---

**Built with FastAPI, Transformers, and PyTorch**
**Ready for production deployment** ðŸš€
